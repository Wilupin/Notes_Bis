\documentclass[10pt]{article}


% Tous les packages prédéfinis
\usepackage{introLatex}
\usepackage{headfootLatex}
\usepackage{shortcutLatex}
\usepackage{envLatex}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
%\usepackage{algpseudocode}

\graphicspath{{logos/}{figures/}}

\newcommand{\gam}[2]{\ensuremath{\Gamma^{(#1)}_{#2}}}
\newcommand{\gamc}[3]{\ensuremath{\Gamma^{(#1)}_{#2}\[#3\]}}
\newcommand{\gamp}[3]{\ensuremath{\Gamma^{(#1)}_{#2}\(#3\)}}
\newcommand{\gamcf}[3]{\ensuremath{\hat{\Gamma}^{(#1)}_{#2}\[#3\]}}
\newcommand{\gampf}[3]{\ensuremath{\hat{\Gamma}^{(#1)}_{#2}\(#3\)}}

\makeatletter
\def\hlinewd#1{%
\noalign{\ifnum0=`}\fi\hrule \@height #1 %
\futurelet\reserved@a\@xhline}
\makeatother





\begin{document}


% Titre du document
\vspace*{-22pt}
\begin{center}
\textbf{\Large Etude mathématique et numérique \\ du groupe de renormalisation non perturbatif}\\
\vspace*{4pt}
Gaétan Facchinetti \\
{\small 27 février - 28 juillet 2017\\
\vspace*{5pt}
\textit{Laboratoire de Physique Théorique de la Matièe Condensée},\\
\textit{Université Paris-Saclay}, \textit{Ecole Normale Supérieure de Cachan}, \\
\textit{Ecole Nationale Supérieure des Techniques Avancées}}\\
\end{center}


\begin{center}
\rule{10cm}{1pt}
\end{center}

%\vspace*{4pt}

\begin{multicols}{2}

\section{Modèle physique}

\subsection{Introduction}

Les transitions de phases sont des phénomènes très étudiés en physique de par l'existence de grandeurs universelles dans leur déroulement \cite{bellac2012}. En effet, pour fixer les idées considérons un système de spins en dimension $d \in \N^*$. Regardons alors une transition d'une phase ferromagnétique (tous les spins alignés dans la même direction) que l'on retrouve à de basses températures à une phase paramagnétique (tous les spins sont désordonnées) qui s'observe à haute température. Dans la classification de Landau cette transition est dite du second ordre. On retrouve donc une phase ordonnée dans laquelle la symétrie du système est brisée, et une phase désordonnée symétrique ; pour passer de l'une a l'autre des deux phases on joue sur la température...


\vspace*{11pt}

\subsection{Description par des champs}

Pour rentrer dans des considérations plus théoriques on considère un système physique en dimension $d$ et symétrique par le groupe de rotation $O(N)$. Il peut alors être décrit par un champ $\varphiv$ de $W^{2,4}(\R^d, \R^N)$. Toutes les informations que l'on peut souhaiter avoir sur ce système sont, dès lors, contenu dans sa fonction de partition dont l'expression purement formelle est 
\begin{equation}
\Zc[\hv] = \int \Dc \varphiv \, e^{-S[\varphiv] + \int_{\R^d} \hv \varphiv} 
\end{equation} 

Où l'intégrale est une intégrale fonctionnelle, sur l'ensemble des champs $\varphiv$ possibles, $\hv$ représente une exitation extérieure et $S[\varphiv]$ est le hamiltonien du système. Dans le modèle $\varphiv^4$ qui ne prend en compte que les termes de plus bas ordre jouant effectivement un role important et compatible avec la symétrie $O(N)$ du système physique, ce hamiltonien s'écrit : 
\begin{equation}
  S[\varphiv] = \int_{\R^d} \left\{ \frac{1}{2} (\nabla \varphiv)^2 + \frac{r_0}{2}\varphiv^2  + \frac{u_0}{4!} {(\varphiv^2)}^2 \right\} \dd^d \xv
\end{equation}

Avec $r_0 = \bar{r_0}(T-T_0)$ un paramètre dépendant linéairement de la température et $u_0$ qui en est indépendant. Cependant, les intégrales fonctionnelles ne sont que des objets formels sur lesquels il est impossible de réaliser une résolution analytique ou numérique directelemnt. L'objectif du groupe de renormalisation est ainsi de parvenir à calculer, sous certaines approximations, des grandeurs ratachées à cette fonction de partition, contenant l'information souhaitée. 

\vspace*{11pt}


\subsection{Grandeurs thermodynamiques}

A partir de la fonction de partition il est possible de regarder un certain nombre de grandeurs thermodynamiques. Tou d'abord on défini l'énergie libre comme étant : 
\begin{equation}
  W[\hv] = \text{ln}{Z[\hv]}
\end{equation}

Par un transformation de Legendre on exprime alors le potentiel de Gibbs
\begin{equation}
  \Gamma [\phiv] = - W[\hv] + \int_{\R^d} \hv \phiv \, \dd^d \xv
\end{equation}

Avec la définition 
\begin{equation}
  \phiv[\xv, \hv] = \left< \varphiv(\xv) \right> = \derd{W[\hv]}{\hv(\xv)}
\end{equation}

On peut alors exprimer simplement la fonction de correlation à $n$ points (grandeur très interessante puisqu'observable expérimentalement) comme étant 
\begin{equation}
  G^{(n)}_{\{i_j\}} [\{\xv_{j}\} ; \hv] = \derd{^n W[\hv]}{h_{i_1}(\xv_1) ... \delta h_{i_n}(\xv_n)}
\end{equation}

De même on défini 
\begin{equation}
  \Gamma^{(n)}_{\{i_j\}} [\{\xv_{j}\} ; \phiv] = \derd{^n \Gamma[\phiv]}{\phi_{i_1}(\xv_1) ... \delta \phi_{i_n}(\xv_n)}
\end{equation}

On peut alors montrer, au sens de l'inverse d'opérateur comme défini en annexe, 
\begin{equation}
  G^{(2)}[\hv] = \(\Gamma^{(2)}[\phiv]\)^{-1}  
\end{equation}


\vspace*{11pt}

\subsection{Théorie de champ moyen, dimension anormale et longueur de Ginzburg}


Une première arppoche pour résoudre ce type de problème est d'aborder une théorie de champ moyen. Pour ce faire on considère $\varphiv_0$ tel que 
\begin{equation}
 \forall i \in \bbrac{1,N} \quad  {\left. \derd{S[\varphiv]}{\varphi_i} \right|}_{\varphiv_0} - h_i = 0
\end{equation}

L'approximation de champ moyen (MF) consiste à écrire une expression approchée de la fonction de partition, comme une approximation de point-col, 
\begin{equation}
  \Zc_{\text{MF}}[\hv] \simeq \exp{\left(-S[\varphiv_0] + \int_{\R^d} \hv \varphiv_0 \dd^d \xv\right)}.
\end{equation}

On considère alors que le champ se trouve dans l'état $\varphiv_0$. Cette approximtion ne tient donc, par définition, compte des fluctuations du champs. Or lorsque l'on étudie leur comportement le théorème de Marmin-Wagner nous montre que pour $d\le 2$ et $N \ge 2$ le système décrit par l'hamiltonien précédent ne pourra pas présenter de transition de phase (le système sera toujours dans une phase désordonnée - symétrique) alors que la théorie de champ moyen en prédit une. Ainsi nous pouvons dire que $d_c^-=2$ est la dimension critique inférieure en dessous de laquelle le modèle champ moyen ne peut être vrai. Dans le cas $N=1$ on montre $d_c^-=1$, (i.e. le modèle d'Ising). \\

On montre qu'il existe aussi une dimension critique supérieur à partir de laquelle on sait que la théorie de champ moyen sera vraie, on la note $d_c^+$. En revanche les choses sont plus compliquées pour ce qu'il se passe entre ces deux dimensions. En effet on va bien se retrouver avec une phase ordonnée mais la théorie de champ moyen ne sera pas en mesure de décrire le comportement critique du système. Des que la longueur de correlation $\xi$ sera supérieure aproximativement à une longueur dite de Ginzburg $\xi_G$, les fluctuations du champs joueront un role non négligeable. On met en évidence expérimentalement que des lors la longueur de correlation évolue en $\xi \sim |T-T_c|^{d-2-\eta}$ avec $\eta \neq 0$ appelée la dimension anormale. La théorie de champ moyen donne, par simple analyse dimensionnelle, obligatoirement $\eta = 0$. 

\vspace*{11pt}

\subsection{Approximation Gaussienne, developpement en nombre de boucles}

\subsubsection{Cas de la phase ordonnée à $\hv=0$}

Dans la phase désordonée ($r_0 < 0$), le champ est constant, on trouve alors que $\varphiv_0^2 = -6r_0/u_0$. Il est possible de développer à l'ordre supérieur la théorie de champ moyen en utilisant l'approximation Gausienne. C'est d'ailleur cette approximation qui nous renseigne quant à l'importance des fluctuations du champ dans le cas $d_c^- < d < d_c^+$. Pour ce faire nous allons utiliser le développement de Taylor fonctionnel de $S$ dans à l'ordre 2, avant de montrer dans un second temps que l'on peut aussi simplement ecrire son expression exacte autour de $\varphiv_0$. On écrit alors $\psiv = \varphiv -\varphiv_0$. Ainsi, en $\mathcal{O}(\psiv^3)$, 
\begin{equation}
  \Zc_G  \propto \int \Dc \psiv \, e^{- \frac{1}{2}\iint \psiv(\xv) S^{(2)}[\varphiv_0, \xv, \yv] \psiv(\yv) \dd^d \xv \dd^d \yv}
\end{equation}

Où l'on a posé
\begin{equation}
S^{(2)}[\varphiv_0, \xv, \yv] = {\left. \derd{^2 S}{\varphiv(\xv)\delta\varphiv(\yv)} \right|}_{\varphiv = \varphiv_0 }
\end{equation}

Posons la nouvelle action
\begin{equation}
 \tilde{S}[\psiv] = \frac{1}{2} \iint \psiv(\xv) S^{(2)}[\varphiv_0, \xv, \yv] \psiv(\yv) \dd^d \xv \dd^d \yv
\end{equation}

Alors, tout calcul fait, 
\begin{equation}
\begin{split}
  \tilde{S}[\psiv] = \int_{\R^d} \left\{ \frac{1}{2} (\nabla \psiv)^2 + \frac{1}{2} r_0 \psiv^2 + \frac{u_0}{4}\varphiv_0^2\psiv^2 \right\} \, \dd^d \xv
\end{split}
\end{equation}

Il vient, avec l'expression de $\varphiv_0^2$
\begin{equation}
  \tilde{S}[\psiv] = \int_{\R^d} \left\{ \frac{1}{2} (\nabla \psiv)^2 - r_0 \psiv^2 \right\} \, \dd^d \xv
\end{equation}

Ainsi la fonction de partition en $\mathcal{O}(\psiv^3)$ est 
\begin{equation}
  \Zc_G \propto \int \Dc \psiv \exp{\left( \int_{\R^d} \left\{ \frac{1}{2} (\nabla \psiv)^2 - r_0 \psiv^2 \right\} \, \dd^d \xv \right)}
\end{equation}

Cependant comme l'expression de $S$ est polynomiale on n'est pas du tout obligé de passé par l'expression de la dérivée fonctionnelle seconde et on peut directement developper $S$ autour de $\varphi_0$. On obtient alors un résultat exact au delà de l'approximation Gaussienne.
\begin{equation}
\begin{split}
  S[\varphiv_0 + \psiv] = \text{Cte} + \int_{\R^d} \left\{ \frac{1}{2} (\nabla \psiv)^2 - r_0 \psiv^2 \right\} + \\ + \int_{\R^d} \left\{ \sqrt{-\frac{r_0 u_0}{6}} \ev_{\varphiv_0} \psiv^3 + \frac{u_0}{4!}\psiv^4 \right\} \, \dd^d \xv
\end{split}
\end{equation}

Où $\ev_{\varphiv_0}$ est le vecteur unitaire direction de $\varphiv_0$.  On remarque alors l'apparition d'un terme cubique qui n'est pas problématique puisque dans la phase ordonnée la symétrie est de toute manière brisée. Pour aller plus loin dans le calcul il est alors possible, par la suite, de faire un développement perturbatif en puissances de $u_0$. 

\vspace*{11pt}

\subsubsection{Cas de la phase désordonnée à $\hv=0$}


Dans le cas de la phase désordonnée ($r_0 > 0$) on a $\varphiv_0 = 0$ dans l'approximation de champ moyen. Il est alors possible de refaire le même raisonnement que précédemment dans ce cas là. L'approximation gaussienne nous donne alors, en $\mathcal{O}(\psiv^3)$
\begin{equation}
  \Zc_G \propto \int \Dc \psiv \exp{\left( \int_{\R^d} \left\{ \frac{1}{2} (\nabla \psiv)^2 + \frac{1}{2} r_0 \psiv^2 \right\} \, \dd^d \xv \right)}
\end{equation}

Comme on pouvait s'y attendre on a simplement ici supprimé la contribution du terme quartique, et l'approximation gaussienne nous ramène au modèle gaussien du modèle. En revanche pour pouvoir aller plus loins, si l'on consièdre la fonction de particion exacte alors il n'y a encore pas d'autre choix ici que de faire un développement perturbatif en puissance de $u_0$.



\vspace*{11pt}

\subsubsection{Cas à $\hv \neq 0$}








\section{Le groupe de renormalisation}

\subsection{L'idée générale}

Comme mentionné dans l'introduction, ce qui nous interesse ici c'est d'étudier le comportement du système lorsque l'on est proche du point critique et donc que les échelles de longeurs qui entrent en jeu deviennent infinies. 

\subsection{Les équations de flot}

\begin{equation}
\partial_t \hat{\Gamma}_{k,ij}[\pv; \phiv] = \frac{1}{2} \int_{\R^d} \partial_t \hat{\Rc}(\qv) {\left(\gamcf{2}{k, ij}{\qv, -\qv; \phiv} + \hat{\Rc}(\qv)\right)}^{-1} \frac{\dd^d \qv}{(2\pi)^d}
\end{equation}


\section{L'approximation BMW}


Il est facile de constater que l'équation de flot obtenue n'est toujours pas soluble numériquement. En effet, \gamp{2}{k,ij}{\pv, \phiv} dépend de \gamp{3}{k,..}{.., \phiv} et \gamp{4}{k, ..}{.., \phiv} et de façon générale on montre facilement que \gam{n}{k} dépend de \gam{n+1}{k} et \gam{n+2}{k}. Nous nous trouvons donc avec un ensemble d'équations couplées ouvert, qui ne peut avoir de solution. Ceci est un résultat général des problèmes à $\mathcal{N}$ corps. L'idée est alors d'utiliser des approximations pour rendre le système d'équation fermé et soluble \cite{Delamotte2012, Ising2DNPRG, Blaizot, Clenshaw, Wetterich, TownsendThesis, Tchebychev, Dupuis2008, LeonardThesis, Onsager}. \\

Une première grande classe d'approximation est appellé l'expension dérivative (DE). On part d'un ansatz pour $\Gamma_k$ choisi comme un développement selon les dérivées de $\phiv$ autour des impulsions faible. L'objectif derrière ceci est de retrouver malgrès une troncation de $\Gamma_k$ la physique du point critique (au faibles impulsions) que l'on veut décrire. Ceci est réalisable en pratique grace à $\mathcal{R}_k$ des équations de flots qui vient supprimer dans les intégrales les grandes impulsions et qui permet d'avoir un dépéndance régulière de $\Gamma_k$ avec l'impulsion. \\

La seconde approximation possible est celle développée par Blaizot-M-W (BMW) et qui permet de ne pas forcément se placerà faible impulsions (et récuperer ainsi plus d'informations physique). 

\newpage

\section{Modèle d'Ising 2D par BMW}

\subsection{Theorie des champs dans un réseau}

On considère le modèle d'Ising classique sur un réseau hypercubique de dimension $d$. La pas $a$ du réseau est exprimé en unité de longueur, on prend $a=1$. On note $\{\ev_\nu\}_{\nu\in\bbrac{1,d}}$ la base cartésienne de ce réseau. Le hamiltonien du système est alors donné par
\begin{equation}
H = -J\beta \sum_{\left<\rv, \rv'\right>}S_\rv S_{\rv'}
\end{equation}
Où $S_\rv$ est la valeur du spin à la position $\rv$, comprise dans $\{-1,1\}$. La notation $\left<\rv, \rv'\right>$ signifie que le terme $S_\rv S_{\rv'}$ contribue à la somme si et seulement si ce sont deux spins plus proches voisins du réseau (i.e s'il existe $\nu \in \bbrac{1,d}$ tel que $\rv' = \rv \pm \ev_\nu$). 

\setlength{\unitlength}{1cm}
\begin{figure}[H]
\begin{center}
\begin{picture}(6,3.8)

\color{cyan}
\put(0,0){\line(1,3){1.2}}
\put(1,0){\line(1,3){1.2}}
\put(2,0){\line(1,3){1.2}}
\put(3,0){\line(1,3){1.2}}
\put(4,0){\line(1,3){1.2}}
\put(5,0){\line(1,3){1.2}}

\put(-0.25,0.45){\line(1,0){5.7}}
\put(0.05,1.25){\line(1,0){5.7}}
\put(0.25,2.0){\line(1,0){5.7}}
\put(0.45,2.70){\line(1,0){5.7}}
\put(0.65,3.35){\line(1,0){5.7}}
\color{red}
\linethickness{0.35mm}


\put(0.15,0.85){\vector(0,-1){0.8}}
\put(1.15,0.05){\vector(0,1){0.8}}
\put(2.15,0.85){\vector(0,-1){0.8}}
\put(3.15,0.05){\vector(0,1){0.8}}
\put(4.15,0.05){\vector(0,1){0.8}}
\put(5.15,0.85){\vector(0,-1){0.8}}

\put(0.4167,1.6){\vector(0,-1){0.7}}
\put(1.4167,0.9){\vector(0,1){0.7}}
\put(2.4167,0.9){\vector(0,1){0.7}}
\put(3.4167,0.9){\vector(0,1){0.7}}
\put(4.4167,1.6){\vector(0,-1){0.7}}
\put(5.4167,0.9){\vector(0,1){0.7}}

\put(0.667,1.7){\vector(0,1){0.6}}
\put(1.667,1.7){\vector(0,1){0.6}}
\put(2.667,2.3){\vector(0,-1){0.6}}
\put(3.667,1.7){\vector(0,1){0.6}}
\put(4.667,1.7){\vector(0,1){0.6}}
\put(5.667,1.7){\vector(0,1){0.6}}

\put(0.9,2.45){\vector(0,1){0.5}}
\put(1.9,2.95){\vector(0,-1){0.5}}
\put(2.9,2.45){\vector(0,1){0.5}}
\put(3.9,2.45){\vector(0,1){0.5}}
\put(4.9,2.95){\vector(0,-1){0.5}}
\put(5.9,2.45){\vector(0,1){0.5}}

\put(1.1167,3.55){\vector(0,-1){0.4}}
\put(2.1167,3.15){\vector(0,1){0.4}}
\put(3.1167,3.15){\vector(0,1){0.4}}
\put(4.1167,3.55){\vector(0,-1){0.4}}
\put(5.1167,3.55){\vector(0,-1){0.4}}
\put(6.1167,3.15){\vector(0,1){0.4}}


\end{picture}
\end{center}
\caption{Exemple de réseau d'Ising en dimension $d=2$. Le système est dans la phase haute température ($T>T_c$).}
\end{figure}


Pour des raisons pratiques on définit maintenant un hamiltonien légerement modifié
\begin{align}
  H_\mu &= -J\beta \sum_{\left<\rv, \rv'\right>}S_\rv S_{\rv'} - \mu \beta N_S \\
  H_\mu  &= -J\beta \sum_{\left<\rv, \rv'\right>}S_\rv S_{\rv'} - \mu \beta \sum_\rv S_\rv^2 
\end{align}

Où $N_S$ est le nombre total de spins. Physiquement cela ne change rien car cela ne fait que décaller l'origine des energies. En revanche cela a un avantage mathématique. En effet, on pose $A_{\rv, \rv'}^{(\mu)}$ la matrice définie implicitement dans $\mathscr{M}_{N_S}(\R)$ par
\begin{equation}
  H_\mu  = -\frac{1}{2} \sum_{\rv, \rv'} S_\rv A_{\rv, \rv'}^{(\mu)}S_{\rv'}
\end{equation}

Il est alors possible de choisir $\mu$ suffisament grand pour que $A_{\rv, \rv'}^{(\mu)}$ soit à diagonale strictement dominante et donc inversible. Par construction il suffit de prendre $\mu > dJ$. On peut alors réaliser une transformée de Hubbard-Stratanovitch. \\

Pour cela commençons par écrire la fonction de partition du modèle
\begin{equation}
  \Zc = \sum_{\{S_\rv\}} e^{-H_{\mu}} =\sum_{\{S_\rv\}}  \exp\(\frac{1}{2} \sum_{\rv, \rv'} S_\rv A_{\rv, \rv'}^{(\mu)}S_{\rv'}\)
\end{equation}

Par integration gaussienne \textit{inverse} il vient, 

\begin{equation}
\begin{split}
  \Zc & \propto \sum_{\{S_\rv\}} \int_\R \prod_{\rv} \, \dd \varphi_\rv \, e^{ -\frac{1}{2} \sum\limits_{\rv, \rv'} \varphi_\rv {(A_{\rv, \rv'}^{(\mu)})}^{-1} \varphi_{\rv'} +\sum\limits_\rv  \varphi_\rv S_\rv  } \\
  \Zc & \propto \int_\R \prod_{\rv} \, \dd \varphi_\rv \, e^{ -\frac{1}{2} \sum\limits_{\rv, \rv'} \varphi_\rv {(A_{\rv, \rv'}^{(\mu)})}^{-1} \varphi_{\rv'} + \sum\limits_\rv \ln\(\cosh(\varphi_\rv)\) } \\
\end{split}
\end{equation}

Cependant nous ne pouvons pas exprimer facilement ${(A_{\rv, \rv'}^{(\mu)})}^{-1}$ et pour cela il est pratique de réaliser une transformée de fourier semi-discrète en posant
\begin{equation}
  \hat{\varphi}(\qv) = \sum_\rv \varphi_\rv e^{-i\qv\rv} \quad \text{et} \quad \varphi_\rv = \int_\qv \hat{\varphi}(\qv)  e^{i\qv\rv}
\end{equation}
Il vient alors
\begin{equation}
\begin{split}
  H_\mu = -J\beta & \sum_{\left<\rv, \rv'\right>} \iint_{\qv,\qv'} \hat{\varphi}(\qv) \hat{\varphi}(\qv')  e^{i(\qv\rv+\qv'\rv')} \\
   -\mu\beta & \sum_\rv \iint_{\qv,\qv'} \hat{\varphi}(\qv) \hat{\varphi}(\qv')  e^{i(\qv+\qv')\rv}
\end{split}
\end{equation}
\commentout{
\begin{equation}
\begin{split}
  H_\mu = -J\beta & \sum_{\rv}\sum_{\nu}   \iint_{\qv,\qv'} \hat{\varphi}(\qv) \hat{\varphi}(\qv')  e^{i((\qv+\qv')\rv \pm \qv'\ev_\nu)} \\
   -\mu\beta & \sum_\rv \iint_{\qv,\qv'} \hat{\varphi}(\qv) \hat{\varphi}(\qv')  e^{i(\qv+\qv')\rv}
\end{split}
\end{equation}
\begin{equation}
\begin{split}
  H_\mu = -J\beta & \iint_{\qv,\qv'} \hat{\varphi}(\qv) \hat{\varphi}(\qv')  \sum_{\rv}  e^{i(\qv+\qv')\rv} \sum_{\nu} e^{ \pm i \qv'\ev_\nu}  \\
   -\mu\beta & \sum_\rv \iint_{\qv,\qv'} \hat{\varphi}(\qv) \hat{\varphi}(\qv')  e^{i(\qv+\qv')\rv}
\end{split}
\end{equation}
}
Notons donc simplement
\begin{equation}
  h_\mu(\qv) = -\beta \left\{ J\sum_{\nu} e^{ \pm i \qv \, \ev_\nu} +\mu \right\} 
\end{equation}
Ainsi, il vient, 
\begin{equation}
  H_\mu =   \iint_{\qv,\qv'} \hat{\varphi}(\qv) \hat{\varphi}(\qv')   h_\mu(\qv')   \sum_{\rv}  e^{i(\qv+\qv')\rv} \\
\end{equation}
\begin{equation}
  H_\mu =   \iint_{\qv,\qv'} \hat{\varphi}(\qv) \hat{\varphi}(\qv')   h_\mu(\qv')   D_{N_S}(\qv+\qv') \\
\end{equation}
En définissant le noyaux de Dirichlet $D_{N_S}$ par
\begin{equation}
  \forall \pv \in [-\pi, \pi]^2 \quad D_{N_S} (\pv) =  \sum_{\rv} e^{i \pv \rv} 
\end{equation}
\commentout{
\begin{equation}
  \begin{split}
  \sum_{x=0}^{N_x} e^{ix(\qv+\qv')_x }  = \frac{1 - e^{i(N_x+1)(\qv+\qv')_x}}{1-e^{i(\qv+\qv')_x }} \\
  = \frac{\sin{\(  \frac{(N_x+1)}{2}(\qv+\qv')_x \)}}{\sin{\( \frac{1}{2}(\qv+\qv')_x \) }} e^{i\frac{N_x}{2}(\qv+\qv')_x }
\end{split}
\end{equation}}
On note $\Dc = \Dc([-\pi, \pi]^2)$, soit $f \in \mathcal{D}$, $D_{N_S}$ étant une fonction de $L^2([-\pi, \pi]^2)$, il appartient à $\Dc'$. On montre alors que \cite{}
\begin{equation}
  \lim\limits_{N_S \rightarrow +\infty} \left< D_{N_S} , f \right>_{\Dc', \Dc} =  \left< \delta , f \right>_{\Dc', \Dc}
\end{equation}
Or comme $$

Ainsi, en prenant la limite $N_S \rightarrow + \infty$ et en raisonnant au sens des distributions,
\begin{equation}
 \lim_{N_S \rightarrow + \infty} H_\mu = -\beta \int_\qv \hat{\varphi}(\qv)  \left\{ \sum\limits_{\nu=1}^{d}  Je^{\pm i\, \qv \, \ev_\nu} + \mu \right\}\hat{\varphi}(-\qv)
\end{equation}
Dans la suite nous ferons l'hypothese, comme souvent en physique, que $N_S$ est suffisament grand pour que l'on écrive, par abus de notation, $H_\mu = \lim\limits_{N_S \rightarrow + \infty} H_\mu$. \\
Nous obtenons alors
\begin{equation}
  H_\mu = -\beta \int_\qv \hat{\varphi}(\qv)  \left\{ J \sum\limits_{\nu=1}^{d} \cos(q_\nu) + \mu \right\}\hat{\varphi}(-\qv)
\end{equation}
Et on en déduit que la transformée de Fourier de $A_{\rv, \rv'}^{(\mu)}$ qui est un opérateur de ... vérifie,
\begin{align}
  \hat{A}(\qv, \qv') = 
  \begin{cases}
    2J\beta \sum\limits_{\nu=1}^{d} \cos(q_\nu) + 2\beta\mu \quad & \text{si } \qv' = - \qv\\
    0 \quad & \text{si } \qv' \neq - \qv
  \end{cases}
\end{align}

Autrement dit, $\hat{A}$ est un operateur diagonal et bien inversible si $\mu > Jd$. Notons,
\begin{align}
\gamma(\qv ) & = \frac{1}{d} \sum\limits_{\nu=1}^{d} \cos(q_\nu) \\
 \lambda_\mu(\qv) & = 2\beta\(J d \gamma(\qv) + \mu\)
\end{align}
En écrivant alors la conservation du produit scalaire de $\mathscr{l}^2$ dans $L^2(\R)$ opérée par la transformation de Fourier semi-discrète nous pouvons écrire : ...

\begin{equation}
  \Zc  \propto \int_\R \prod_{\rv} \, \dd \varphi_\rv \, e^{-S_\mu[\varphi] } \\
\end{equation}
Avec l'action $S$ s'écrivant :
\begin{equation}
  S_\mu[\varphi] = \frac{1}{2} \int_\qv \varphi(\qv) \frac{1}{\lambda_\mu(\qv)} \varphi(-\qv) - \sum\limits_\rv \ln\(\cosh(\varphi_\rv)\)
\end{equation}
Par isometrie de la transformation de Fourier, et donc par le théorème de Parseval, nous réecrivons $S$ sous la forme 
\begin{equation}
  \begin{split}
    S_\mu[\varphi] & = \frac{1}{2} \int_\qv \varphi(\qv) \[\frac{1}{\lambda_\mu(\qv)} - \frac{1}{\lambda_\mu(0)}\] \varphi(-\qv) \\
    &+ \sum\limits_\rv \[\frac{1}{2\lambda_\mu(0)}\varphi_\rv^2 - \ln\(\cosh(\varphi_\rv)\) \]
  \end{split}
\end{equation}
Enfin, soit $\delta \in \R^+_*$, on pose le changement de variable, 
\begin{equation}
  \varphi \rightarrow \delta\sqrt{2 \beta J d} \, \varphi 
\end{equation}
On obtient alors 
\begin{equation}
S_\mu[\varphi] = \frac{1}{2} \int_\qv \hat{\varphi}(\qv)\eo(\qv)\hat{\varphi}(-\qv) + \sum_\rv V_0(\varphi(\rv))
\end{equation}
Avec, en posant $\tilde{\mu} = \mu/(Jd)$ et $\tilde{\beta} = \beta Jd$,
\begin{equation}
  \eo(\qv) = \delta^2\frac{1 - \gamma(\qv)}{(\gamma(\qv) + \tilde{\mu})(1+\tilde{\mu})}
\end{equation}
\begin{equation}
  V_0(\rho) = \delta^2 \frac{1}{1+\tilde{\mu}} \rho - \ln\(\cosh\(2\delta\sqrt{\tilde{\beta}\rho}\)\)
\end{equation}

On se retrouve donc ici avec la formultion d'un problème de théorie des champs que l'on peut résoudre avec le groupe de renormalisation non perturbatif et nottamment avec l'approximation BMW.\\

De plus, on note $\tilde{\beta}_c^{\text{MF}}$ la valeur de $\tilde{\beta}$ en champ moyen à la temperature critique. On peut recuperer cette valeur en résolvant $V_0 = 0$ pour $\rho \neq 0$ tendant vers $0$. En faisant un développement limité à l'ordre 1 en $\rho$ nous avons
\begin{equation}
V_0(\rho) = \delta^2\( \frac{1}{1+\tilde{\mu}} - 2\tilde{\beta}\)\rho + \mathcal{O}(\rho^2)
\end{equation}
Ainsi, nous obtenons 
\begin{equation}
\tilde{\beta}_c^{\text{MF}} \simeq \frac{1}{2(1+\tilde{\mu})}
\end{equation}

Enfin, nous avons besoin, dans les équations, de la dérivée du potentiel et non pas vraiment du potentiel. Or la fonction $V_0$ est dérivable sur $]0, +\infty[$. Ainsi, pour $\rho \neq 0$ 
\begin{equation}
  W_0(\rho) = \partial_\rho V_0 (\rho) = \frac{\delta^2}{1+\tilde{\mu}}-\delta\sqrt{\frac{\tilde{\beta}}{\rho}}\tanh{\(2\delta\sqrt{\tilde{\beta\rho}}\)}
\end{equation}
Et par un développement limitée en 0, en prolongeant $W_0$ par continuité
\begin{equation}
  W_0(0) \equiv \delta^2\( \frac{1}{1+\tilde{\mu}} - 2\tilde{\beta}\)
\end{equation}

\vspace*{11pt}


\subsection{Les équations BMW en $\rho$}

Les équations à résoudre numériquement sont

\begin{equation}
\begin{split}
\partial_t & \Delta_k(p_x, p_y, \rho)  = - 2\rho I_3(\rho) u_k^2(\rho) \\
& + 2\rho J_3(p_x, p_y, \rho) {\[ u_k(\rho) + \partial_\rho \Delta_k(p_x, p_y, \rho) \]}^{2} \\
& - \frac{1}{2}I_2(\rho)\[\partial_\rho \Delta_k(p_x, p_y, \rho) + 2\rho\partial_\rho^2 \Delta_k(p_x, p_y, \rho) \]
\end{split} 
\end{equation}
\begin{equation}
\partial_t W_k(\rho) = \frac{1}{2} \partial_\rho I_1 (\rho)
\end{equation}

Avec les notations

\end{multicols}
\noindent
\rule{9cm}{1pt}
\begin{equation}
J_n (p_x, p_y, \rho) = \frac{1}{(2\pi)^2} \int_{-\pi}^\pi \int_{-\pi}^\pi \partial_t \Rc_k(q_x, q_y) \,
G_k^{n-1}(q_x,q_y,\rho)G_k(p_x+q_x,p_y+q_y,\rho) \, \dd q_x \,\dd q_y
\end{equation}
\begin{equation}
I_n (\rho) = \frac{1}{(2\pi)^2} \int_{-\pi}^\pi \int_{-\pi}^\pi \partial_t \Rc_k(q_x, q_y) \,
G_k^{n}(q_x,q_y,\rho) \, \dd q_x \,\dd q_y
\end{equation}
\begin{equation}
G_k(q_x,q_y,\rho) = \frac{1}{\eo(q_x, q_y) + \Delta_k(q_x, q_y, \rho) + m^2_k(\rho) + \Rc_k(q_x, q_y) }
\end{equation}
\begin{equation}
\partial_\rho I_n(\rho) = - n \frac{1}{(2\pi)^2} \int_{-\pi}^\pi \int_{-\pi}^\pi \partial_t \Rc_k(q_x, q_y) G_k^{n+1}(q_x,q_y,\rho)\(\partial_\rho \Delta_k(p_x, p_y, \rho) + u_k(\rho) \) \, \dd q_x \,\dd q_y
\end{equation}
\vspace*{11pt}
\hfill \rule{9cm}{1pt}
\begin{multicols}{2}

Mais aussi en reprenant 

\begin{equation}
m_k^2 (\rho) = \partial_\phi^2 V(\phi) = W(\rho) + 2\rho\partial_\rho W(\rho)
\end{equation}
\begin{equation}
u_k(\rho) = \partial_\rho m_k^2(\rho) = 3\partial_\rho W(\rho) + 2\rho\partial_\rho^2 W(\rho)
\end{equation}


Pour le regulateur nous avons opté pour une expression exponentielle dont on a déjà mis en évidence qu'elle était stable pour les équations BMW.
On pose,
\begin{equation}
  \tau(q_x, q_y) = \frac{\eo(q_x, q_y)}{2k^2 {\|\eo\|}_{\infty}}
\end{equation}

On choisit alors
\begin{equation}
  \Rc_k(q_x, q_y) = \frac{\alpha \eo(q_x, q_y)}{\exp\(2\tau(q_x, q_y)\) -1 }
\end{equation}
\begin{equation}
  \partial_t \Rc_k(q_x, q_y) = \alpha \eo(q_x, q_y) \frac{\tau(q_x, q_y)}{\sinh^2\(\tau(q_x, q_y)\)}
\end{equation}

Et nous pouvons calculer
\begin{equation}
  {\|\eo\|}_{\infty} = \underset{(p_x, p_y) \in [-\pi,\pi]^2}{\text{sup}} \eo(p_x, p_y) = \frac{2\delta^2}{\mu^2 -1}
\end{equation} 


\vspace*{11pt}


\subsection{Les équations BMW en $\phi$}

On peut repréndre les équations précédentes mais les écrire cette fois-ci directement selon la variable $\phi$. On doit alors résoudre

\begin{equation}
\begin{split}
\partial_t & \Delta_k (p_x, p_y, \phi) = - I_3(\phi){(\partial_\phi X(\phi))}^2 \\
& + J_3(p_x, p_y, \phi) {\(\partial_\phi \left\{ \Delta_k (p_x, p_y, \phi) + X(\phi) \right\}\)}^2 \\
& - \frac{1}{2} I_2(\phi) \partial_\phi^2 \Delta_k(p_x, p_y, \phi) 
\end{split}
\end{equation}
\begin{equation}
\partial_t X(\phi) = \frac{1}{2} \partial_\phi^2 I_1(\phi)
\end{equation}

On garde ici les meme expressions que pécédemment en changeant $\rho$ en $\phi$. 


\end{multicols}
\noindent
\rule{9cm}{1pt}
\begin{equation}
J_n (p_x, p_y, \phi) = \frac{1}{(2\pi)^2} \int_{-\pi}^\pi \int_{-\pi}^\pi \partial_t \Rc_k(q_x, q_y) \,
G_k^{n-1}(q_x,q_y,\phi)G_k(p_x+q_x,p_y+q_y,\phi) \, \dd q_x \,\dd q_y
\end{equation}
\begin{equation}
I_n (\phi) = \frac{1}{(2\pi)^2} \int_{-\pi}^\pi \int_{-\pi}^\pi \partial_t \Rc_k(q_x, q_y) \,
G_k^{n}(q_x,q_y,\phi) \, \dd q_x \,\dd q_y
\end{equation}
\begin{equation}
G_k(q_x,q_y,\phi) = \frac{1}{\eo(q_x, q_y) + \Delta_k(q_x, q_y, \phi) + X(\phi) + \Rc_k(q_x, q_y) }
\end{equation}
\begin{equation}
\partial_\phi I_n(\phi) = - n \frac{1}{(2\pi)^2} \int_{-\pi}^\pi \int_{-\pi}^\pi \partial_t \Rc_k(q_x, q_y) G_k^{n+1}(q_x,q_y,\phi)\(\partial_\phi \Delta_k(p_x, p_y, \phi) + \partial_\phi X(\phi) \) \, \dd q_x \,\dd q_y
\end{equation}
\begin{equation}
\begin{split}
\partial_\phi^2 I_n(\phi) = & - n \frac{1}{(2\pi)^2} \int_{-\pi}^\pi \int_{-\pi}^\pi \partial_t \Rc_k(q_x, q_y) G_k^{n+1}(q_x,q_y,\phi)\(\partial_\phi^2 \Delta_k(p_x, p_y, \phi) + \partial_\phi^2 X(\phi) \) \, \dd q_x \,\dd q_y \\
& + n(n+1) \frac{1}{(2\pi)^2} \int_{-\pi}^\pi \int_{-\pi}^\pi \partial_t \Rc_k(q_x, q_y) G_k^{n+2}(q_x,q_y,\phi){\(\partial_\phi \Delta_k(p_x, p_y, \phi) + \partial_\phi X(\phi) \)}^2 \, \dd q_x \,\dd q_y
\end{split}
\end{equation}
\vspace*{11pt}
\hfill \rule{9cm}{1pt}
\begin{multicols}{2}




\subsection{Adimenssionement en impulsions}

On note $\tp_x = k^{-1}p_x$ et $\tp_y = k^{-1}p_y$. Ainsi que
\begin{center}
$\tDelta_k(\tp_x, \tp_y,\phi) = \Delta_k(p_x, p_y,\phi)$ \\
$\tJ_n(\tp_x, \tp_y,\phi) = J_n(p_x, p_y,\phi)$ \\
$\tRc_k(\tp_x, \tp_y) = \Rc_k(p_x, p_y) $ \\
$\teo(\tp_x, \tp_y) = \eo(p_x, p_y) $ \\
\end{center}

Les équations se réécrivent
\begin{equation}
\begin{split}
\partial_t & \tDelta_k (\tp_x, \tp_y, \phi) = - I_3(\phi){(\partial_\phi X(\phi))}^2 \\
& + \tJ_3(\tp_x, \tp_y, \phi) {\(\partial_\phi \left\{ \tDelta_k (\tp_x, \tp_y, \phi) + X(\phi) \right\}\)}^2 \\
& - \frac{1}{2} I_2(\phi) \partial_\phi^2 \tDelta_k(\tp_x, \tp_y, \phi) + \tp_x \partial_{\tp_x}  \tDelta_k + \tp_y \partial_{\tp_y}  \tDelta_k 
\end{split}
\end{equation}
\begin{equation}
\partial_t X(\phi) = \frac{1}{2} \partial_\phi^2 I_1(\phi)
\end{equation}

En effet, par transformée de legendre on a : 
\begin{equation}
  \begin{split}
    \partial_t \Delta_k {|}_{p_x, p_y, \phi} & = \partial_t \tDelta_k {|}_{\tp_x, \tp_y, \phi}  \\
    & + \partial_t \tp_x  {|}_{p_x} \partial_{\tp_x}  \tDelta_k + \partial_t \tp_y  {|}_{p_y} \partial_{\tp_y}  \tDelta_k
  \end{split}
\end{equation}
Ce qui donne alors 
\begin{equation}
  \begin{split}
    \partial_t \Delta_k {|}_{p_x, p_y, \phi} =   \partial_t \tDelta_k {|}_{\tp_x, \tp_y, \phi}  & -  \tp_x \partial_{\tp_x}  \tDelta_k -  \tp_y \partial_{\tp_y}  \tDelta_k 
  \end{split}
\end{equation}
De même on a 
\begin{equation}
  \begin{split}
    \partial_t \Rc_k {|}_{q_x, q_y} =   \partial_t \tRc_k {|}_{\tq_x, \tq_y}  -  \tq_x \partial_{\tq_x}  \tRc_k  -  \tq_y \partial_{\tq_y}  \tRc_k 
  \end{split}
\end{equation}
En outre,
\end{multicols} 
\noindent
\rule{9cm}{1pt}
\begin{equation}
  \begin{split}
    \partial_{\tq_x}  \tRc_k = \alpha \frac{\partial_{\tq_x} \teo}{2} \( \frac{1}{\sinh(\ttau)\exp(\ttau)} - \frac{\ttau}{\sinh^2(\ttau)}\) 
  \end{split}
\end{equation}
\begin{equation}
  \begin{split}
    \partial_{t}  \tRc_k  = \alpha \frac{\partial_{t} \teo}{2} \( \frac{1}{\sinh(\ttau)\exp(\ttau)} - \frac{\ttau}{\sinh^2(\ttau)}\) 
     + \alpha \teo \frac{\ttau}{\sinh^2(\ttau)}
  \end{split}
\end{equation}
Ainsi en rassemblant les différentes équations précédentes :
\begin{equation}
  \begin{split}
    \partial_{t}  \Rc_k  = \alpha\frac{1}{2} \( \frac{1}{\sinh(\ttau)\exp(\ttau)} - \frac{\ttau}{\sinh^2(\ttau)}\) \( \partial_{t} \teo  - \partial_{\tq_x} \teo - \partial_{\tq_y} \teo \) 
     + \alpha \teo \frac{\ttau}{\sinh^2(\ttau)}
  \end{split}
\end{equation}
Avec les relations
\begin{equation}
  \begin{split}
    \partial_{\tq_x} \teo(\tq_x, \tq_y) = k \partial_{q_x} \eo(q_x, q_y)  = \frac{2 \delta^2 k \sin(k\tq_x)}{(\cos(k\tq_x)+\cos(k\tq_y)+2\mu)^2}
  \end{split}
\end{equation}
\begin{equation}
  \begin{split}
    \partial_{t} \teo(\tq_x, \tq_y) = \frac{2 \delta^2 k  \(\tq_x\sin(k\tq_x)+\tq_y\sin(k\tq_y) \)}{(\cos(k\tq_x)+\cos(k\tq_y)+2\mu)^2}
  \end{split}
\end{equation}
Ce qui donne pour expression de la parenthese,
\begin{equation}
  \begin{split}
 \partial_{t} \teo  - \partial_{\tq_x} \teo - \partial_{\tq_y} \teo = 2 \delta^2 k \frac{  (\tq_x-1)\sin(k\tq_x)+(\tq_y-1)\sin(k\tq_y)}{(\cos(k\tq_x)+\cos(k\tq_y)+2\mu)^2}
  \end{split}
\end{equation}
Avec les intégrales qui se calculent selon
\begin{equation}
I_n (\phi) = \frac{k^2}{(2\pi)^2} \int_{-\frac{\pi}{k}}^{\frac{\pi}{k}} \int_{-\frac{\pi}{k}}^{\frac{\pi}{k}}\( \partial_t \tRc_k (\tq_x, \tq_y)  -  \tq_x \partial_{\tq_x}  \tRc_k(\tq_x, \tq_y)  -  \tq_y \partial_{\tq_y}  \tRc_k(\tq_x, \tq_y) \) \,
\tG_k^{n}(\tq_x,\tq_y,\phi) \, \dd \tq_x \,\dd \tq_y
\end{equation}
\begin{equation}
\tG_k(\tq_x,\tq_y,\phi) = \frac{1}{\teo(\tq_x, \tq_y) + \tDelta_k(\tq_x, \tq_y, \phi) + X(\phi) + \tRc_k(\tq_x, \tq_y) }
\end{equation}
\vspace*{11pt}
\hfill \rule{9cm}{1pt}
\begin{multicols}{2}

\section{Schéma numérique}

Nous savons que ce qui est important ici est d'avoir un schéma avec des calculs de dérivées suffisaments précis pour ne pas avoir d'instabilité numériques qui peuvent se propager très vite, d'autant plus que l'on utilise un simple schéma d'Euler. L'idée est alors d'utiliser la même technique de simulation ici en 2D qui ce qui a été fait en 1D pour les équations BMW du modèle $\phiv^4$.

\subsection{Intégration}

Pour calculer numériquement les différentes intégrales nous nous servons des propriétées de symétries des différentes fonctions. En effet pour tout $\rho \in \R^+$ les fonctions $\eo(.,.)$, $\Rc_k(.,.)$, $\Delta_k(.,.,\rho)$, ...

\subsubsection{Integration d'une fonction symétrique}

Soit $f$ une fonction de $[-\pi, \pi] \times [-\pi, \pi]$ à valeurs dans $\R$. On suppose que $f$ est symétrique par rapport à l'axe des $x=0$, à l'axe des $y=0$ et à l'axe $x=y$. Nous cherchons alors une quadrature pour integrer $f$ sur son domaine de définition. Pour commencer, en utilisant les deux premières symétries,
\begin{equation}
\int_{-\pi}^{\pi} \int_{-\pi}^{\pi} f(x,y) \, \dd x \, \dd y = 4 \int_{0}^{\pi}  \int_{0}^{\pi} f(x,y) \, \dd x \, \dd y
\end{equation} 
On peut alors faire le changement de variable affine  $\(\tilde{x}, \tilde{y}\) \rightarrow \(2x/\pi -1, 2y/\pi -1\) $ donnant,  
\begin{equation}
\begin{split} 
  \int_{0}^{\pi}  \int_{0}^{\pi}  f(x,y) \, & \dd x \, \dd y = \\
 \frac{\pi^2}{4} \int_{-1}^{1} & \int_{-1}^{1} f\(\frac{\pi}{2}\(\tilde{x}+1\),\frac{\pi}{2}\(\tilde{y}+1\)\) \, \dd \tilde{x}  \, \dd \tilde{y} 
\end{split}
\end{equation}
Les intégrales sur le carré unité $[-1,1] \times [-1, 1]$ sont alors calculées avec une quadrature de Gauss-Legendre \textit{tensorielle} obtenue à partir de la quadrature de Gauss-Legendre 1D. Il vient des considérations précédentes, 
\begin{equation}
\begin{split}
 \int_{-\pi}^{\pi} \int_{-\pi}^{\pi} f(x,y) \,&  \dd x \, \dd y \simeq \\ 
 \pi^2\sum_{i=0}^{N_{GL}} & \sum_{j=0}^{N_{GL}}w_i w_j f(\frac{\pi}{2}\(\xi_i+1\), \frac{\pi}{2}\(\xi_j+1\))
\end{split}
\end{equation}
Où $\{w_i\}_{i \in \bbrac {1, N_{GL}}}$ sont les poids d'integration de Gauss Legendre en 1D et  $\{\xi_i\}_{i \in \bbrac {1, N_{GL}}}$ les points d'intégration correspondants. Par construction, $((w_i w_j))_{i,j}$ est une matrice symétrique et par symétrie de $f$ par rapport à la première bissectrice nous pouvons alors réduire la double somme par, 
\begin{equation}
\begin{split}
 \int_{-\pi}^{\pi} \int_{-\pi}^{\pi} f(x,y) \,&  \dd x \, \dd y \simeq \\ 
 \pi^2\sum_{i=0}^{N_{GL}} & \sum_{j=0}^{i-1}w_i w_j 2 f(\frac{\pi}{2}\(\xi_i+1\), \frac{\pi}{2}\(\xi_j+1\)) \, + \\
\pi^2\sum_{i=0}^{N_{GL}} & w_i^2f(\frac{\pi}{2}\(\xi_i+1\), \frac{\pi}{2}\(\xi_i+1\))
\end{split}
\end{equation}
Ce qui permet de réduire le temps de calcul en réduisant légèrement la complexité algorithmique.

\vspace*{11pt}




\subsubsection{Integration d'une fonction presque symétrique et $2\pi$ périodique}

Soit $(a,b) \in [0,\pi]\times [0,\pi]$. Soit maintenant $f$ et $h$ deux fonctions de $\R^2$ dans $\R$ et  $g$ définie sur  $[-\pi, \pi] \times [-\pi, \pi]$ par $g : (x,y) \rightarrow f(x+a, y+b)$. On suppose, comme précédemment, que $f$ et $h$ sont symétriques par rapport à l'axe $x=0$, à l'axe $y=0$ et à l'axe $x=y$. On fait aussi l'hypothèse qu'elles sont $2\pi$ periodiques. Nous cherchons alors une quadrature pour integrer $g\times h$ sur $[-\pi, \pi] \times [-\pi, \pi]$. Pour cela remarquons que
\begin{equation}
\begin{split}
 & \int_{-\pi}^{\pi} \int_{-\pi}^{\pi} g(x,y) h(x,y) \,  \dd x \, \dd y  = \\
 \int_{0}^{\pi} & \int_{0}^{\pi} [f(x+a, y+b) + f(x+a, y-b)] h(x,y) \, \dd x \, \dd y \, + \\ 
\int_{0}^{\pi} & \int_{0}^{\pi} [f(x-a, y+b) + f(x-a, y-b)] h(x,y)  \, \dd x \, \dd y 
\end{split}
\end{equation}
On peut alors adopter la même quadrature que précédement, cependant ici 

\vspace*{11pt}


\subsection{Interpolation}


Afin de pouvoir calculer nos intgérales comme précédemment avec une bonne préceision et notamment pour obtenir $J_3$, il est necessaire d'interpoler correctement les fonctions dans le plan $\Pc$.

\subsubsection{Quelques théorèmes et définitions}

Nous rappelons ici quelques théorèmes utiles pour justifier l'emploi des expressions développées dans les paragaraphes précédents. L'ensemble de ces théorèmes et de leurs démonstrations ce trouve dans l'ouvrage ... 

\begin{theorem}
  Soit $f : [-1,1]^2 \rightarrow \C$ une foncion continue aux variations bornées ... On suppose que l'une des dérivées partielles de de $f$ existe et est bornée dans $[-1,1]$. Alors $f$ admet un développment de Chebychev de la forme 
\begin{equation}
f(x,y) = \sum_{i=0} ^{\infty} \sum_{j=0} ^{\infty} c_{ij} T_i(x)T_j(y)
\end{equation}
De plus, la série ... converge uniformément vers $f$.
\end{theorem}




\begin{definition}
  Un opérateur tensoriel $\mathcal{L}$ est un opérateur linéaire agissant sur des fonctions de deux variables tel que si $f(x,y) = g(x)h(y)$ alors $\mathcal{L}(f) = \mathcal{L}_x(g)\mathcal{L}_y(h)$. Avec $ \mathcal{L}_x$ et $\mathcal{L}_y$ des opérateurs quelconques. 
\end{definition}

\begin{proposition}{Algorithme de Clenshaw}.
Soit, de manière générale, une suite de polynômes $\{\Pc_n\}_{n \in \N}$ liés par la relation
\begin{equation}
  \forall x \in \R \quad \Pc_{n+1}(x) = u_n(x)\Pc_n(x) + v_n(x)\Pc_{n-1}(x)
\end{equation}
On souhaite calculer, pour $x \in \R$ donné, 
\begin{equation}
  S = \sum_{l=0}^{N} a_l \Pc_l(x)
\end{equation}
On considère l'algorithme suivant :
\vspace*{-11pt}
\begin{algorithm}[H]
  \begin{algorithmic}[1]
    \STATE $b_{N+2} = 0$ ; $b_{N+1} = 0$
    \FOR{ $m = N .. 1$  }
    \STATE $b_m = a_m + u_m(x)b_{m+1} + v_{m+1}(x)b_{m+2}$
    \ENDFOR
    \STATE $S_1 = a_0\Pc_0(x) + b_1\Pc_1(x) + b_2v_1(x)\Pc_0(x)$
  \end{algorithmic}
\end{algorithm}
\vspace*{-11pt}
\noindent
Alors nous avons $S = S_1$. 
\end{proposition}

\begin{proof}
Ce résultat ce démontre par récurrence sur $m \in \bbrac{1,N}$ en remarquant que,
\begin{equation}
S_1 = S_m +  b_{m+1}\Pc_{m+1}(x) + b_{m+2}v_{m+1}(x)\Pc_{m+2}(x)
\end{equation}
Avec la notation $S_m = \sum\limits_{l=0}^{m} a_l \Pc_l(x)$.
\end{proof}

L'algorithme qui vient de la démonstration précédente a l'avantage qu'il ne faut, pour l'utiliser, connaitre que les deux premiers termes de la suite de polynômes. Appliqué aux polynômes de Tchebytchev il est possible de remarquer que l'implémentation de cette algorithme peut se faire avec $3(N +1)$ opérations, alors qu'il faut $4N-2$ opérations pour un algorithme calculant la somme étape par étape en se servant de la relation de récurrence des polynômes de Tchebythev (appellé algorithme de Forsythe). Aisni pour $N \ge 5$ (ce qui est toujours le cas dans les simulations que ous avons réalisées) l'algorithme de Clenshaw est moins couteux. De plus il a été montré ... que l'erreur produite par ce clacul est équivalente à celle produite par l'algorithme de Forsythe. 



\subsubsection{Methode 1 : decomposition directe}

L'hypothèse fondamentale pour que notre simulation se comporte correctement et que toutes les fonctions avec lesquelles on travaille soient régulières. Ce qui est en fait assuré par le régulateur que l'on introduit. Aisni si l'on considère une fonction générique $f$ régulière (i.e. satisfaiant les hypothèses du théorème ...), alors on sait qu'elle se décompose en série de Chebychev. On chcerche une décomposition tronquée de la forme :

\begin{equation}
  f(x,y) \simeq \sum_{i=0}^{N_C-1}\sum_{j=0}^{N_C-1} c_{ij} T_i(x)T_j(y)
\end{equation}
Pour trouver la valeur des coefficients de la matrice $((c_{i,j}))_{i,j}$ on prend les ensembles des racines du polynôme de Chebychev  de degré $N_C$, $\left\{x_m\right\}_{0\le m \le N_C-1}$ et $\left\{y_n\right\}_{0\le m \le N_C-1}$ et on impose,
\begin{equation}
  f(x_m, y_n) = \sum_{i=0}^{N_C-1}\sum_{j=0}^{N_C-1} c_{ij} T_i(x_m)T_j(y_n)
\end{equation}
En utilisant les relations des polyômes de Chebychev on peut alors determiner les coefficients de la matrice $((c_{ij}))_{i,j}$ avec les formules
\begin{equation}
 c_{ij} = \frac{A}{N_C^2} \sum_{m=0}^{Nc-1} \sum_{n=0}^{Nc-1} f(x_m, y_n)T_i(x_m)T_j(y_n)
\end{equation}
\begin{align}
  A = 
  \begin{cases}
    1 \quad \text{si } i = j = 0 \\
    2 \quad \text{si } i = 0 \text{ et } j \neq 0 \\
    2 \quad \text{si } i \neq 0 \text{ et } j = 0 \\
    4 \quad \text{si } i \neq 0 \text{ et } j \neq 0 \\
  \end{cases}
\end{align}

Cette méthode est extrêmement couteuse puisque pour calculer l'ensemble des coefficients de $((c_{ij}))_{i,j}$ cela demande un algorithme de complexité évlouant en $\mathcal{O}(N_c^2)$ et dejà long pour de faibles valeurs de $N_c$. Ce pourquoi, afin d'obtenir de meilleures précisions sur les intégrales calculées tout en conservant un temps de calcul raisonnable nous avons implémenté la deuxième méthode suivante.


\subsubsection{Methode 2 : approximation de rang faible}

La méthode suivante est inspirée de ce qui est mis en place dans le paquet \textit{chebfun} développé par ... qui permet justement de traiter les interpolations de Chebychev sous Matlab. Au lieu de faire directement une décomposition sur une base tensorielle de polynomes de Chebychev on commence par réaliser une approcximation de rang faible de la fonction que l'on souhaite approximer. Plus précisemment, on considère les ensembles des racines de Chebychev $\left\{x_m\right\}_{0\le m \le N_C-1}$ et $\left\{y_n\right\}_{0\le m \le N_C-1}$. Alors on peut former la matrice suivante $\mat{F} =((f(x_m, y_n ))_{0\le m,n \le N_C-1}$. On peut faire de cette matrice une approximation de rang faible par élimination Gaussienne.

\begin{algorithm}[H]
  \begin{algorithmic}[1]
    \STATE Initialisation : $\mat{E}^0 = 0$; $\mat{F}_0 = 0$; $k = 1$;
    \WHILE{ ${\| \mat{E}^k \|}_{\infty} < \varepsilon $ }
    \STATE $(i_k, j_k) =  {\text{argmax}}_{(i,j)} \left\{\left| \mat{E}^{k-1}_{i,j} \right| \right\}$
    \STATE $\mat{C}^k_{j} = \mat{E}^k_{i_k,j}$;  $\mat{R}^k_{i} = \mat{E}^k_{i,j_k}$; $d_k = \mat{E}^k_{i_k,j_k}$
    \STATE $\mat{E}^k_{i,j} = \mat{E}^{k-1}_{i,j} - d_k^{-1}\mat{C}^k_{j}\mat{R}^k_{i}$
    \STATE $\mat{F}^k_{i,j} = \mat{F}^{k-1}_{i,j} + d_k^{-1}\mat{C}^k_{j}\mat{R}^k_{i}$
    \ENDWHILE
  \end{algorithmic}
\end{algorithm}

Notons $Q$ le rang de l'approximation obetnue. On obtient alors une écriture de $f$ sous la forme 
On obtient alors une expression de la forme 
\begin{equation}
\mat{F} \simeq \tilde{\mat{F}} = \sum_{j=1}^Q d_j \mat{C}^j \mat{R}^j
\end{equation}
Comme nous ne conaissons exactement la fonctions $f$ qu'aux points d'interpolation de Chebychev cela revient au même que d'écrire que nou avons décomposé $f$ comme une somme de produits de fonctions à une variable,
\begin{equation}
f(x,y) \simeq \sum_{j=1}^Q d_jc^j(y)r^j(x)
\end{equation}
On peut alors décomposer les fonctions $c_j$ et $r_j$ sur une base de polynômes de Chebychev comme on peut le faire pour toute fonction d'une seule variable. La décomposition est ainsi un une opération de produit tensoriel. \\
En outre, remarquons que si jamais 



\subsection{La dérivation}

Dans les expressions que l'on obtient il est nécessaire de dériver des fonctions par rapport à une variable qui est discrétisée sur une grille régulière. Pour cela nous utilisons des schémas de différences finies. Comme cela avait été fait pour les codes écrits jusqu'à présent, nous avons repris des schémas à 5 points pour le calcul des dérivées (ou des schémas à 3 points). En outre, comme nous ne disposans d'aucune condition aux limites nous sommes contraints pour pouvoir avoir une solution de calculer les dérivées des points en bords de grille en utilisant des schémas décalés n'utilsant que des points compris dans la grille. Les schémas utilisés sont les suivants. On considère une fonction $u$ d'une variable en espace définie sur $[0, +\infty[$, discrétisée sur une grille régulière de pas $h$. On note $u_j = u(jh)$ pour $j \in \bbrac{0, j_{\text{max}}}$.\\

\end{multicols}

Schéma centré :
\begin{align*}
  \partial_xu (jh) &= \frac{1}{12h}\(u_{j-2} - 8u_{j-1} + 8u_{j+1} - u_{j+2}\) \\
  \partial_x^2 u (jh) &= \frac{1}{12h^2}\( -u_{j-2} + 16u_{j-1} - 30u_{j+1} -u_{j+2}\)
\end{align*}

Schémas décentrés gauches : 
\begin{align*}
  \partial_xu (jh) &= \frac{1}{12h}\( -25u_{j} + 48u_{j+1} -36u_{j+2} + 16u_{j+3} - 3u_{j+4} \) \\
  \partial_x^2 u (jh) &= \frac{1}{12h^2} \( 35u_{j} -104u_{j+1} + 114u_{j+2} - 56u_{j+3} + 11u_{j+4}\)\\
  \partial_xu (jh) &= \frac{1}{12h}\( -3u_{j-1} - 10u_{j} +18u_{j+1} - 6u_{j+2} + u_{j+3} \) \\
  \partial_x^2 u (jh) &= \frac{1}{12h^2}\( 11u_{j-1} - 20u_{j} + 6u_{j+1} + 4u_{j+2} - u_{j+3} \) \\
\end{align*}

Schémas décentrés droits : 
\begin{align*}
  \partial_xu (jh) &= \frac{1}{12h}\( 25u_{j} - 48u_{j-1} + 36u_{j-2} - 16u_{j-3} + 3u_{j-4} \) \\
  \partial_x^2 u (jh) &= \frac{1}{12h^2} \( 35u_{j} -104u_{j-1} + 114u_{j-2} - 56u_{j-3} + 11u_{j-4}\)\\
  \partial_xu (jh) &= \frac{1}{12h}\( 3u_{j+1} + 10u_{j} - 18u_{j-1} + 6u_{j-2} - u_{j-3} \) \\
  \partial_x^2 u (jh) &= \frac{1}{12h^2}\( 11u_{j-1} - 20u_{j} + 6u_{j+1} + 4u_{j+2} - u_{j+3} \) \\
\end{align*}

\begin{multicols}{2}

\subsection{Dichotomie}

Le programme tourne pour une valeur de $\tilde{\beta}$ initiale donnée. A la fin (i.e. au bout d'un temps de RG suffisamment long) on regarde la valeur de $W(0)$ si jamais elle est positive alors on a fini dans la phase symétrique et la température initiale supérieure à la température critique (resp.  $\tilde{\beta}$ inférieur à  $\tilde{\beta}_c$). Si jamais la valeur de $W(0)$ est négative nous sommes dans la phase brisée basse température et la temperatur initiale était inférieure à la température critique. On choisit donc de tester par dichotomie de façon à se rapprocher le plus possible de la température critique les valeurs initiale de $\tilde{\beta}$ telles que 
\setlength{\unitlength}{1cm}
\begin{center}
\begin{picture}(6,1)
\put(0,0){\vector(1,0){6}}
\put(1,-0.25){\line(0,1){0.5}}
\put(0.8,-0.6){$T_c$}
\put(2.8,-0.25){\line(0,1){0.5}}
\put(2.65,-0.6){$T_i$}
\put(5,-0.25){\line(0,1){0.5}}
\put(4.8,-0.6){$T_c^{MF}$}
\end{picture}
\end{center}

\begin{center}
\begin{picture}(6,1)
\put(6,0){\vector(-1,0){6}}
\put(1,-0.25){\line(0,1){0.5}}
\put(0.8,-0.6){$\beta_c$}
\put(2.8,-0.25){\line(0,1){0.5}}
\put(2.65,-0.6){$\beta_i$}
\put(5,-0.25){\line(0,1){0.5}}
\put(4.8,-0.6){$\beta_c^{MF}$}
\end{picture}
\end{center}

\vspace*{11pt}



\bibliographystyle{plain}
\bibliography{notes}




\newpage
\null
\newpage

\appendix




















\section{Rappels de calculs}

\subsection{Derivation fonctionnelle}


\textbf{Définition}\\
Soit $U$ et $V$ deux espaces de Banach. Soit $F$ une fonctionnelle de $U$ dans $V$. 
Soit $f \in U$. On appelle, si elle existe, dérivée (au sens de Fréchet) de la fonctionelle $F$ prise en $f$, l'application linéaire de $\mathcal{L}(U,V)$, notée $D_fF$ telle que, pour $\eps>0$, 

\begin{equation}
\forall h \in U \quad \lim_{ \eps \rightarrow 0 } \frac{\| F[f+\eps h] - F[f] - \eps D_fF.h\|_V}{\eps}  = 0
\end{equation} 


Dans notre étude nous recontrerons essentiellement $V = (\R, |.|)$. On utilisera alors aussi les crochets de dualités, qui permettent de réécrire l'action d'un élément $T$ de $U^*$ sur $U$ (et de les identifier) : 

\begin{equation}
  \forall h \in U \quad T.h = \left< T, h \right>_{U^*, U}
\end{equation}

Ainsi, si la dérivée au sens de Fréchet de $F$ existe (donc si $F$ est Frechet différentlable en $f$) alors on note $\derd{F}{f}$ l'application de $U^*$ définie par 

\begin{equation}
  \forall h \in U \quad D_fF.h = \left< \derd{F}{f}, h \right>_{U^*, U}
\end{equation}

\vspace*{11pt}
\subsection{Derivation fonctionnelle}


\textbf{Définition}\\
Soit $U$ et $V$ deux espaces de Banach. Soit $F$ une fonctionnelle de $U$ dans $V$. 
Soit $f \in U$. On appelle, si elle existe, dérivée (au sens de Fréchet) de la fonctionelle $F$ prise en $f$, l'application linéaire de $\mathcal{L}(U,V)$, notée $D_fF$ telle que, pour $\eps>0$, 

\begin{equation}
\forall h \in U \quad \lim_{ \eps \rightarrow 0 } \frac{\| F[f+\eps h] - F[f] - \eps D_fF.h\|_V}{\eps}  = 0
\end{equation} 


Dans notre étude nous recontrerons essentiellement $V = (\R, |.|)$. On utilisera alors aussi les crochets de dualités, qui permettent de réécrire l'action d'un élément $T$ de $U^*$ sur $U$ (et de les identifier) : 

\begin{equation}
  \forall h \in U \quad T.h = \left< T, h \right>_{U^*, U}
\end{equation}

Ainsi, si la dérivée au sens de Fréchet de $F$ existe (donc si $F$ est Frechet différentlable en $f$) alors on note $\derd{F}{f}$ l'application de $U^*$ définie par 

\begin{equation}
  \forall h \in U \quad D_fF.h = \left< \derd{F}{f}, h \right>_{U^*, U}
\end{equation}

\vspace*{11pt}
\subsection{Derivation fonctionnelle}


\textbf{Définition}\\
Soit $U$ et $V$ deux espaces de Banach. Soit $F$ une fonctionnelle de $U$ dans $V$. 
Soit $f \in U$. On appelle, si elle existe, dérivée (au sens de Fréchet) de la fonctionelle $F$ prise en $f$, l'application linéaire de $\mathcal{L}(U,V)$, notée $D_fF$ telle que, pour $\eps>0$, 

\begin{equation}
\forall h \in U \quad \lim_{ \eps \rightarrow 0 } \frac{\| F[f+\eps h] - F[f] - \eps D_fF.h\|_V}{\eps}  = 0
\end{equation} 


Dans notre étude nous recontrerons essentiellement $V = (\R, |.|)$. On utilisera alors aussi les crochets de dualités, qui permettent de réécrire l'action d'un élément $T$ de $U^*$ sur $U$ (et de les identifier) : 

\begin{equation}
  \forall h \in U \quad T.h = \left< T, h \right>_{U^*, U}
\end{equation}

Ainsi, si la dérivée au sens de Fréchet de $F$ existe (donc si $F$ est Frechet différentlable en $f$) alors on note $\derd{F}{f}$ l'application de $U^*$ définie par 

\begin{equation}
  \forall h \in U \quad D_fF.h = \left< \derd{F}{f}, h \right>_{U^*, U}
\end{equation}

\vspace*{11pt}
\vspace*{11pt}
\subsection{Derivation fonctionnelle}


\textbf{Définition}\\
Soit $U$ et $V$ deux espaces de Banach. Soit $F$ une fonctionnelle de $U$ dans $V$. 
Soit $f \in U$. On appelle, si elle existe, dérivée (au sens de Fréchet) de la fonctionelle $F$ prise en $f$, l'application linéaire de $\mathcal{L}(U,V)$, notée $D_fF$ telle que, pour $\eps>0$, 

\begin{equation}
\forall h \in U \quad \lim_{ \eps \rightarrow 0 } \frac{\| F[f+\eps h] - F[f] - \eps D_fF.h\|_V}{\eps}  = 0
\end{equation} 


Dans notre étude nous recontrerons essentiellement $V = (\R, |.|)$. On utilisera alors aussi les crochets de dualités, qui permettent de réécrire l'action d'un élément $T$ de $U^*$ sur $U$ (et de les identifier) : 

\begin{equation}
  \forall h \in U \quad T.h = \left< T, h \right>_{U^*, U}
\end{equation}

Ainsi, si la dérivée au sens de Fréchet de $F$ existe (donc si $F$ est Frechet différentlable en $f$) alors on note $\derd{F}{f}$ l'application de $U^*$ définie par 

\begin{equation}
  \forall h \in U \quad D_fF.h = \left< \derd{F}{f}, h \right>_{U^*, U}
\end{equation}

\vspace*{11pt}
\vspace*{11pt}
\subsection{Derivation fonctionnelle}


\textbf{Définition}\\
Soit $U$ et $V$ deux espaces de Banach. Soit $F$ une fonctionnelle de $U$ dans $V$. 
Soit $f \in U$. On appelle, si elle existe, dérivée (au sens de Fréchet) de la fonctionelle $F$ prise en $f$, l'application linéaire de $\mathcal{L}(U,V)$, notée $D_fF$ telle que, pour $\eps>0$, 

\begin{equation}
\forall h \in U \quad \lim_{ \eps \rightarrow 0 } \frac{\| F[f+\eps h] - F[f] - \eps D_fF.h\|_V}{\eps}  = 0
\end{equation} 


Dans notre étude nous recontrerons essentiellement $V = (\R, |.|)$. On utilisera alors aussi les crochets de dualités, qui permettent de réécrire l'action d'un élément $T$ de $U^*$ sur $U$ (et de les identifier) : 

\begin{equation}
  \forall h \in U \quad T.h = \left< T, h \right>_{U^*, U}
\end{equation}

Ainsi, si la dérivée au sens de Fréchet de $F$ existe (donc si $F$ est Frechet différentlable en $f$) alors on note $\derd{F}{f}$ l'application de $U^*$ définie par 

\begin{equation}
  \forall h \in U \quad D_fF.h = \left< \derd{F}{f}, h \right>_{U^*,


\subsection{Transformées de Fourier}

On rappelle ici les notations utilisée pour définir les transformées de Fourier. Pour cela on considère $f$ une application de $L^2(\R^d, \R^N)$. On définit alors la transformée de Fourier de $f$, notée $\hat{f}$ par, 

\begin{equation}
  \forall q \in \R^d \quad \hat{f}(q) = \frac{1}{\sqrt{(2\pi)^d}}\int_{\R^d} f(x) e^{-iqx}\dd x
\end{equation}

Nous avons alors la relation inverse,

\begin{equation}
  \forall x \in \R^d \quad f(x) = \frac{1}{\sqrt{(2\pi)^d}}\int_{\R^d} \hat{f}(q) e^{iqx}\dd q
\end{equation}

\textbf{Remarque 1} \\
Dans le cas ou l'on a des fonctions définies non pas sur $\R^d$ mais sur un domaine $\Omega \subset \R^d$ fini alors nous aurons les  mêmes propriétés avec la relation "d'équivalence" : 

\begin{equation}
  \frac{1}{(2\pi)^d} \int_{\R^d} \dd^d \qv \overset{\text{$\Omega$ fini}}{\quad \longrightarrow \quad } \frac{1}{\Omega} \sum_{\qv}
\end{equation}


\textbf{Remarque 2} \\
Dans le cadre où $f \in \Sr'(\R^d, \R^N)$ (espace des distributions tempérées) alors on étend la notion de transformée de Fourier $\hat{f}$ de $f$ à l'aide du crochet de dualité, 
\begin{equation}
  \forall \phi \in \Sr(\R^d, \R^N) \quad \left< \hat{f}, \phi \right>_{\Sr', \Sr} = \left< f, \hat{\phi} \right>_{\Sr', \Sr}
\end{equation}

\vspace*{11pt}







\vspace*{11pt}

\subsection{TF et dérivées fonctionnelles}


On expose ici la justification d'un résultat très souvent utilisé dans la dérivation des équations du groupe de renormalisation non perturbatif.\\ 

%\noident
\textbf{Proposition}

Soit $F$ une fonctionelle de $U$ dans $(\R,|.|)$ et $f \in U$.
Soit $x \in \R^d$. On suppose que $U = L^2(\R^d, \R^N)$. 
Alors, 
\begin{equation}
  \derd{F}{f(x)} = \frac{1}{\sqrt{(2\pi)^d}} \int_{\R^d} \derd{F}{\hat{f}(-q)} e^{iqx} \dd^d q
\end{equation} 


\vspace*{11pt}
%\noindent
\textbf{Demonstration}

Par la règle de la chaine de la dérivation de Fréchet nous pouvons écrire, 
\begin{equation}
  \forall h \in U \quad D_fF.h = D_{\hat{f}}F. D_f \hat{f}.h
\end{equation}


\commentout{
Ainsi, ceci nous donne donc, pour $x \in \R^d$
\begin{equation}
  \derd{F}{f(x)} = \int_{\R^d} \derd{\hat{f}(q)}{f(x)}\derd{F}{\hat{f}(q)} \dd^d q
\end{equation}
}

Cependant, $\hat{f}$ est une fonctionnelle de $f$ (par définition de la TF) de $U$ dans $U$ qui est linéaire en $f$. Soit $\eps >0$, 
\begin{equation}
  \forall h \in U \quad \hat{f}[f + \eps h] - \hat{f}[f] = \eps \hat{h} 
\end{equation} 

Il vient directement, par définition de la dérivation au sens de Frechet, $D_f\hat{f} .h = \hat{h}$.  
Ainsi,  $D_f F .h = D_{\hat{f}}F.\hat{h}$.  On peut alors écrire, 
\begin{align}
  \forall h \in U  \quad D_f F .h & = \int_{\R^d} \derd{F}{f(q)} \int_{\R^d} h(x) e^{-iqx} \dd^d x \dd^d q \\
  \forall h \in U  \quad D_f F .h & = \int_{\R^d} \int_{\R^d} \derd{F}{f(q)} e^{-iqx} h(x) \dd^d q \dd^d x \\
  \forall h \in U  \quad D_f F .h & = \int_{\R^d} \int_{\R^d} \derd{F}{f(-q)} e^{iqx} \dd^d q \text{ } h(x) \dd^d x
\end{align}

Ce qui permet de conclure la démonstration.

\vspace*{11pt}

\end{multicols}


\pagebreak

\section{Opérateur à noyaux}

Nous détaillons dans cette sections quelques propriétés élémentaires mais très utiles dans notre études des opérateurs à noyaux. Après une définition rigoureuse de ces opérateurs et de leur trace nous utiliserons une description plus formelle de ce que peut être leur inverse.\\



\subsection{Definition}

Soit $S$ un endomorphisme de . On appelle $S$ un operateur a noyaux, une application de $L^2(\R^d, \R^N)$, telle que pour tout $(i,j) \in \bbrac{1,N}^2$, il existe une application $A_{i,j} \in L^2(\R^d\times\R^d, \R)$ telle que,  pour tout $f \in L^2(\R^d, \R^N)$, et $x \in \R^d$ 
 \begin{equation}
  S[f]_i(x) = \int_{\R^d} \sum_{j=1}^{N} A_{i,j}(x,y)f_j(y) \dd^d y
 \end{equation}

 La matrice $A : (x,y) \rightarrow ((A_{i,j}(x,y)))_{i,j}$ est appellée noyaux de $S$. \\
 On identifiera alors dans les notations $S$ et $A$ indépendemment. \\

\textbf{Proposition}\\
Par Cauchy-Schwartz cette définition à un sens, $S$ est bien définie. \\
De plus $S$ et est un endomorphisme de $L^2(\R^d, \R^N)$. \\

\textbf{Démonstration}\\
Soit $f\in L^2(\R^d, \R^N)$.\\

\begin{equation}
   \int_{\R^d} \left| S[f]_i (\xv) \right|^{2} \dd^d \xv \le \int_{\R^d} \int_{\R^d} \sum_j 
\end{equation}


\vspace*{11pt}

\subsection{TF d'un opérateur à noyaux}

Nous avons vu que $S$ définie comme précédemment était un endomorphisme de $L^2(\R^d, \R^N)$. Ainsi il est possible d'en définir la transformée de Fourier. \\

\textbf{Proposition}\\
Pour $S$ opérateur à noyaux défini comme ci-dessus, nous avons,
\begin{equation}
  \hat{S}[f]_i(\qv) = \int_{\R^d} \sum_{j=1}^{N} \hat{A}_{i,j}(\qv, -\qv') \hat{f}_j(\qv')
\end{equation}



\textbf{Démonstration}\\
Nous développons le calcul de la transformée de Fourier. Soit $\qv \in \R^d$,
\begin{align}
 \hat{S}[f]_i(\qv)  = & \int_{\R^d} \int_{\R^d} \sum_{j=1}^{N} A_{i,j}(\xv,\yv)f_j(\yv) e^{-i\qv\xv} \dd^d \yv \dd^d \xv \\
 \hat{S}[f]_i(\qv)  = & \int_{\R^{3d}} \sum_{j=1}^{N} A_{i,j}(\xv,\yv) \hat{f}_j(\qv') e^{i\qv'\yv} e^{- i\qv\xv}  \dd^d \qv' \dd^d \yv \dd^d \xv \\
 \hat{S}[f]_i(\qv)  = &    \sum_{j=1}^{N} \int_{\R^d} \hat{f}_j(\qv') \left\{ \int_{\R^{2d}} A_{i,j}(\xv,\yv)  e^{i\qv'\yv} e^{- i\qv\xv}  \dd^d \yv \dd^d \xv   \right\} \dd^d \qv'\\
\hat{S}[f]_i(\qv)  = &    \sum_{j=1}^{N} \int_{\R^d} \hat{f}_j(\qv') \hat{A}_{i,j}(\qv, -\qv') \dd^d \qv'\\
\end{align}

\vspace*{11pt}

\subsection{Produit de Volterra}

Considérons deux opérateurs à noyaux, $S$ et $T$ de noyaux respectifs $A$ et $B$. \\
La composition de ces deux opérateur s'écrit comme suit,
\begin{equation}
  \forall \xv \in \R^d \quad S \circ T [f]_i (\xv) = \int_{\R^d}  \sum_{j=1}^{N} \left\{ \int_{\R^d} \sum_{j=k}^{N}A_{i,k}(\xv, \zv)B_{k,j}(\zv, \yv)  \dd^d \yv \right\} f_j(\yv)  \dd^d \yv
\end{equation}

On notera alors aussi
\begin{equation}
    \forall (\xv, \yv) \in (\R^d)^2 \quad (A \circ B)_{i,j} (\xv, \yv) = \int_{\R^d} \sum_{j=k}^{N}A_{i,k}(\xv, \zv)B_{k,j}(\zv, \yv)  \dd^d \yv 
\end{equation}

\textbf{Proposition}\\
Par transformée de Fourier, par un calcul anaolgue à celui fait pour un simple noyaux, il vient,
\begin{equation}
\forall (\qv, \qv') \in (\R^d)^2 \quad\widehat{(A \circ B)}_{i,j}(\qv, \qv') =\int_{\R^d} \sum_{j=k}^{N}\hat{A}_{i,k}(\qv, \qv'')\hat{B}_{k,j}(-\qv'', \qv')  \dd^d \qv'' 
\end{equation}



\vspace*{11pt}

\subsection{Trace d'un opérateur à noyaux}

\textbf{Definition}\\
On définit la trace d'un opérateur $S$ de noyaux $A$ par
\begin{equation}
  \text{Tr} A = \sum_{i=1}^{N} \int_{\R^d} A_{i,i}(\xv,\xv) \dd^d \xv 
\end{equation}


\textbf{Proposition}\\
Nous pouvons de manière similaire à ce qui a été fait pour démontrer l'expression de la transformée de Fourier d'un opérateur à noyaux en (...) dériver la relation sur la trace de la transformée de Fourier
\begin{equation}
  \text{Tr} \hat{A} = \sum_{i=1}^{N} \int_{\R^d} \hat{A}_{i,i}(\qv,-\qv) \dd^d \qv 
\end{equation}



\vspace*{11pt}



\subsection{Dérivation de l'inverse d'une matrice}

Afin de s'approprier les propriétés des opérateurs à noyaux, de par leur similarités avec les matrices nous commenceront par développer de petites propriétés matricielles que nous pourrons transposer par la suite.

\vspace*{11pt}

\textbf{Proposition} \\
Soit $ n \in \mathbb{N}$. Soit $((A_{i,j}))_{i,j}$ une matrice de $\R^{n \times n}$ telles que pour tout $(i,j) \in \bbrac{1,n}$, $A_{i,j}$ soit une application de $\mathscr{C}^1(\R,\mathbb{C}) $. On suppose que pour tout $x \in \R$, $A(x) \in  \mathbb{G}L_n(\R)$. Alors, 
\begin{equation}
  \forall x \in \R \quad \frac{dA^{-1}(x)}{dx} = - A^{-1}(x) \frac{dA(x)}{dx}A^{-1}(x)
\end{equation} 


\textbf{Demonstration}\\
Soit $x \in \R$.
Par définition de $A^{-1}$, $AA^{-1} = \text{Id}_{\R^{n\times n}}$.\\
En dérivant cette relation nous obtenons, 
\begin{equation}
  \frac{dA(x)}{dx}A^{-1}(x) + A\frac{dA^{-1}(x)}{dx} = 0
\end{equation}

Il vient directement le résultat.



\section{Formules en vrac}

\begin{equation}
   \delta(\xv - \yv) = \int_{\R^d} F(\xv, \zv) F^{-1}(\zv, \yv) \dd^d \zv
\end{equation}

\begin{equation}
   \delta_{i,j} \delta(\xv - \yv) = \int_{\R^d} \sum_{k=1}^{N} F_{i,k}(\xv, \zv) F^{-1}_{k,j}(\zv, \yv) \dd^d \zv
\end{equation}

\begin{equation}
   \delta(\qv_1 +\qv_3) = \int_{\R^d} \hat{F}(\qv_1, -\qv_2) \hat{F}^{-1}(\qv_2, \qv_3) \dd^d \qv_3
\end{equation}


\begin{equation}
   \delta_{i,j}\delta(\qv_1 +\qv_3) = \int_{\R^d} \sum_{k=1}^{N} \hat{F}_{i,k}(\qv_1, -\qv_2) \hat{F}^{-1}_{k,j}(\qv_2, \qv_3) \dd^d \qv_3
\end{equation}

\begin{equation}
   \derd{\hat{F}}{\hat{\phiv}(-\pv)}(\qv_1, \qv_4) = - \int_{\R^d} \int_{\R^d} \hat{F}^{-1}(\qv_1, -\qv_2) \derd{\hat{F}}{\hat{\phiv}(-\pv)}(\qv_2, \qv_3) \hat{F}^{-1}(-\qv_3, \qv_4) \dd^d \qv_2 \dd^d \qv_3
\end{equation}










\pagebreak

\begin{multicols}{2}

\section{Methode formelle de recherche des exposants critiques}

Soit $U \in D = H_1(]-\infty, 0], H_2(\R))$, avec $H_1$ et $H_2$ deux espaces de Hilbert. Soit $f$ un opérateur agissant de $D$ dans $\R$. On suppose que $\text{Ker}f$ est réduit à un singleton (hypothèse fondamentale du RG). On suppose de plus que $f$ dépend continument de la fonction en laquelle elle est évaluée (autre hypotèse du RG). On note l'équation de flot de $U$ :
\begin{equation}
  \forall (t, x) \in ]-\infty, 0]\times\R \quad \partial_t U(t,x) = f[U(t, x)]
\end{equation}

On note point fixe de $U$ la grandeur $U^*=\text{Ker}f$. De l'équation précédente, il vient directement que $\partial_t U^*(t, x) = 0$ et aisni $U^*$ ne dépend pas de $t$. De plus par l'hypothèse de continuité nous pouvons linéaiser l'équation précédente autour du point fixe à l'ordre 1. Soit $(t, x) \in ]-\infty, 0]\times\R$, on note $\delta U(t,x) = U(t,x) - U^*(x)$ : 

\begin{equation}
  \partial_t \delta U(t,x) \simeq \int_{\R} {\left. \derd{f[U]}{U(t,x)} \right|}_{U(t,x) = U^*(x)}  \delta U(t, x) \dd x 
\end{equation}

On pose un nouvel opérateur A défini de $D$ dans $\R$ : 

\begin{equation}
  A = \int_{\R} {\left. \derd{f[U]}{U(t,x)} \right|}_{U(t,x) = U^*(x)} \quad . \quad \dd x
\end{equation} 

Ainsi nous supposons maintenant que $A$ est diagonalisable dans $H_2$ et possède un spectre discret de valeurs propres réelles. De plus on suppose que les vecteurs propres forment une base hilbertienne de l'espace considéré. Ainsi nous posons $\left\{ \lambda_i\right\}_{i\in\N}$ l'ensemble des valeurs propres rangées dans l'ordre décroissant et $\left\{ w_i\right\}_{i\in\N}$ l'ensemble des vecteurs propres associées. Alors, 
\begin{equation}
  \delta U(t,x) = \sum_{i\in\N} (\delta U(t, .),  w_i(.))_{H_2} w_i(x).
\end{equation}

On note $h_i : t \rightarrow (\delta U(t, .),  w_i(.))_{H_2}$. Ainsi,
\begin{equation}
  \sum_{i\in\N} \partial_t h_i(t) w_i(x) = \sum_{i\in\N} \lambda_i  h_i(t) w_i(x)
\end{equation}

Il vient alors 
\begin{equation}
  \forall i \in \N \quad h_i(t) = h_i(0) e^{-\lambda_i t}.
\end{equation}

D'ou l'expression de la solution, 
\begin{equation}
  U(t,x) = U^*(x) + \sum_{i\in \N} h_i(0)e^{-\lambda_i t}w_i(x).
\end{equation}


En pratique la fonction $U$ aura été echantillonée pour pouvoir adopter une résolution numérique. Les opérateurs devienne alors des matrices. De plus dans les faits, nous ne pouvons pas appliquer un tel formalisme et la méthode sera utilisée puis justifiée a posteriori par les resultats trouvés. En effet, par exemple, rien ne nous assure la diagonalisabilité du spectre de $A$ ainsi que les propriétés supposée de son spectre de manière générale.\\

Ce sont les valeurs propres trouvées et notamment celles qui sont positives qui nous permettent de determiner un certains nombre des exposants critiques. Nous


\end{multicols}


\end{document}
